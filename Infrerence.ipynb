{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "from typing import List, NamedTuple\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "assert tf.__version__.startswith('2')\n",
    "from tflite_support import metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Interpreter = tf.lite.Interpreter\n",
    "load_delegate = tf.lite.experimental.load_delegate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectDetectorOptions(NamedTuple):\n",
    "  \"\"\"A config to initialize an object detector.\"\"\"\n",
    "\n",
    "  enable_edgetpu: bool = False\n",
    "  \"\"\"Enable the model to run on EdgeTPU.\"\"\"\n",
    "\n",
    "  label_allow_list: List[str] = None\n",
    "  \"\"\"The optional allow list of labels.\"\"\"\n",
    "\n",
    "  label_deny_list: List[str] = None\n",
    "  \"\"\"The optional deny list of labels.\"\"\"\n",
    "\n",
    "  max_results: int = -1\n",
    "  \"\"\"The maximum number of top-scored detection results to return.\"\"\"\n",
    "\n",
    "  num_threads: int = 1\n",
    "  \"\"\"The number of CPU threads to be used.\"\"\"\n",
    "\n",
    "  score_threshold: float = 0.0\n",
    "  \"\"\"The score threshold of detection results to return.\"\"\"\n",
    "\n",
    "\n",
    "class Rect(NamedTuple):\n",
    "  \"\"\"A rectangle in 2D space.\"\"\"\n",
    "  left: float\n",
    "  top: float\n",
    "  right: float\n",
    "  bottom: float\n",
    "\n",
    "\n",
    "class Category(NamedTuple):\n",
    "  \"\"\"A result of a classification task.\"\"\"\n",
    "  label: str\n",
    "  score: float\n",
    "  index: int\n",
    "\n",
    "\n",
    "class Detection(NamedTuple):\n",
    "  \"\"\"A detected object as the result of an ObjectDetector.\"\"\"\n",
    "  bounding_box: Rect\n",
    "  categories: List[Category]\n",
    "\n",
    "\n",
    "def edgetpu_lib_name():\n",
    "  \"\"\"Returns the library name of EdgeTPU in the current platform.\"\"\"\n",
    "  return {\n",
    "      'Darwin': 'libedgetpu.1.dylib',\n",
    "      'Linux': 'libedgetpu.so.1',\n",
    "      'Windows': 'edgetpu.dll',\n",
    "  }.get(platform.system(), None)\n",
    "\n",
    "\n",
    "class ObjectDetector:\n",
    "  \"\"\"A wrapper class for a TFLite object detection model.\"\"\"\n",
    "\n",
    "  _OUTPUT_LOCATION_NAME = 'location'\n",
    "  _OUTPUT_CATEGORY_NAME = 'category'\n",
    "  _OUTPUT_SCORE_NAME = 'score'\n",
    "  _OUTPUT_NUMBER_NAME = 'number of detections'\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      model_path: str,\n",
    "      options: ObjectDetectorOptions = ObjectDetectorOptions()\n",
    "  ) -> None:\n",
    "    \"\"\"Initialize a TFLite object detection model.\n",
    "    Args:\n",
    "        model_path: Path to the TFLite model.\n",
    "        options: The config to initialize an object detector. (Optional)\n",
    "    Raises:\n",
    "        ValueError: If the TFLite model is invalid.\n",
    "        OSError: If the current OS isn't supported by EdgeTPU.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load metadata from model.\n",
    "    displayer = metadata.MetadataDisplayer.with_model_file(model_path)\n",
    "\n",
    "    # Save model metadata for preprocessing later.\n",
    "    model_metadata = json.loads(displayer.get_metadata_json())\n",
    "    process_units = model_metadata['subgraph_metadata'][0]['input_tensor_metadata'][0]['process_units']\n",
    "    mean = 0.0\n",
    "    std = 1.0\n",
    "    for option in process_units:\n",
    "      if option['options_type'] == 'NormalizationOptions':\n",
    "        mean = option['options']['mean'][0]\n",
    "        std = option['options']['std'][0]\n",
    "    self._mean = mean\n",
    "    self._std = std\n",
    "\n",
    "    # Load label list from metadata.\n",
    "    file_name = displayer.get_packed_associated_file_list()[0]\n",
    "    label_map_file = displayer.get_associated_file_buffer(file_name).decode()\n",
    "    label_list = list(filter(lambda x: len(x) > 0, label_map_file.splitlines()))\n",
    "    self._label_list = label_list\n",
    "\n",
    "    # Initialize TFLite model.\n",
    "    if options.enable_edgetpu:\n",
    "      if edgetpu_lib_name() is None:\n",
    "        raise OSError(\"The current OS isn't supported by Coral EdgeTPU.\")\n",
    "      interpreter = Interpreter(\n",
    "          model_path=model_path,\n",
    "          experimental_delegates=[load_delegate(edgetpu_lib_name())],\n",
    "          num_threads=options.num_threads)\n",
    "    else:\n",
    "      interpreter = Interpreter(\n",
    "          model_path=model_path, num_threads=options.num_threads)\n",
    "\n",
    "    interpreter.allocate_tensors()\n",
    "    input_detail = interpreter.get_input_details()[0]\n",
    "\n",
    "    # From TensorFlow 2.6, the order of the outputs become undefined.\n",
    "    # Therefore we need to sort the tensor indices of TFLite outputs and to know\n",
    "    # exactly the meaning of each output tensor. For example, if\n",
    "    # output indices are [601, 599, 598, 600], tensor names and indices aligned\n",
    "    # are:\n",
    "    #   - location: 598\n",
    "    #   - category: 599\n",
    "    #   - score: 600\n",
    "    #   - detection_count: 601\n",
    "    # because of the op's ports of TFLITE_DETECTION_POST_PROCESS\n",
    "    # (https://github.com/tensorflow/tensorflow/blob/a4fe268ea084e7d323133ed7b986e0ae259a2bc7/tensorflow/lite/kernels/detection_postprocess.cc#L47-L50).\n",
    "    \n",
    "    sorted_output_indices = sorted(\n",
    "        [output['index'] for output in interpreter.get_output_details()])\n",
    "    self._output_indices = {\n",
    "        self._OUTPUT_LOCATION_NAME: sorted_output_indices[0],\n",
    "        self._OUTPUT_CATEGORY_NAME: sorted_output_indices[1],\n",
    "        self._OUTPUT_SCORE_NAME: sorted_output_indices[2],\n",
    "        self._OUTPUT_NUMBER_NAME: sorted_output_indices[3],\n",
    "    }\n",
    "\n",
    "    self._input_size = input_detail['shape'][2], input_detail['shape'][1]\n",
    "    self._is_quantized_input = input_detail['dtype'] == np.uint8\n",
    "    self._interpreter = interpreter\n",
    "    self._options = options\n",
    "\n",
    "  def detect(self, input_image: np.ndarray) -> List[Detection]:\n",
    "    \"\"\"Run detection on an input image.\n",
    "    Args:\n",
    "        input_image: A [height, width, 3] RGB image. Note that height and width\n",
    "          can be anything since the image will be immediately resized according\n",
    "          to the needs of the model within this function.\n",
    "    Returns:\n",
    "        A Person instance.\n",
    "    \"\"\"\n",
    "    image_height, image_width, _ = input_image.shape\n",
    "\n",
    "    input_tensor = self._preprocess(input_image)\n",
    "\n",
    "    self._set_input_tensor(input_tensor)\n",
    "    self._interpreter.invoke()\n",
    "\n",
    "    # Get all output details\n",
    "    boxes = self._get_output_tensor(self._OUTPUT_LOCATION_NAME)\n",
    "    classes = self._get_output_tensor(self._OUTPUT_CATEGORY_NAME)\n",
    "    scores = self._get_output_tensor(self._OUTPUT_SCORE_NAME)\n",
    "    count = int(self._get_output_tensor(self._OUTPUT_NUMBER_NAME))\n",
    "\n",
    "    return self._postprocess(boxes, classes, scores, count, image_width,\n",
    "                             image_height)\n",
    "\n",
    "  def _preprocess(self, input_image: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Preprocess the input image as required by the TFLite model.\"\"\"\n",
    "\n",
    "    # Resize the input\n",
    "    input_tensor = cv2.resize(input_image, self._input_size)\n",
    "\n",
    "    # Normalize the input if it's a float model (aka. not quantized)\n",
    "    if not self._is_quantized_input:\n",
    "      input_tensor = (np.float32(input_tensor) - self._mean) / self._std\n",
    "\n",
    "    # Add batch dimension\n",
    "    input_tensor = np.expand_dims(input_tensor, axis=0)\n",
    "\n",
    "    return input_tensor\n",
    "\n",
    "  def _set_input_tensor(self, image):\n",
    "    \"\"\"Sets the input tensor.\"\"\"\n",
    "    tensor_index = self._interpreter.get_input_details()[0]['index']\n",
    "    input_tensor = self._interpreter.tensor(tensor_index)()[0]\n",
    "    input_tensor[:, :] = image\n",
    "\n",
    "  def _get_output_tensor(self, name):\n",
    "    \"\"\"Returns the output tensor at the given index.\"\"\"\n",
    "    output_index = self._output_indices[name]\n",
    "    tensor = np.squeeze(self._interpreter.get_tensor(output_index))\n",
    "    return tensor\n",
    "\n",
    "  def _postprocess(self, boxes: np.ndarray, classes: np.ndarray,\n",
    "                   scores: np.ndarray, count: int, image_width: int,\n",
    "                   image_height: int) -> List[Detection]:\n",
    "    \"\"\"Post-process the output of TFLite model into a list of Detection objects.\n",
    "    Args:\n",
    "        boxes: Bounding boxes of detected objects from the TFLite model.\n",
    "        classes: Class index of the detected objects from the TFLite model.\n",
    "        scores: Confidence scores of the detected objects from the TFLite model.\n",
    "        count: Number of detected objects from the TFLite model.\n",
    "        image_width: Width of the input image.\n",
    "        image_height: Height of the input image.\n",
    "    Returns:\n",
    "        A list of Detection objects detected by the TFLite model.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Parse the model output into a list of Detection entities.\n",
    "    for i in range(count):\n",
    "      if scores[i] >= self._options.score_threshold:\n",
    "        y_min, x_min, y_max, x_max = boxes[i]\n",
    "        bounding_box = Rect(\n",
    "            top=int(y_min * image_height),\n",
    "            left=int(x_min * image_width),\n",
    "            bottom=int(y_max * image_height),\n",
    "            right=int(x_max * image_width))\n",
    "        class_id = int(classes[i])\n",
    "        category = Category(\n",
    "            score=scores[i],\n",
    "            label=self._label_list[class_id],  # 0 is reserved for background\n",
    "            index=class_id)\n",
    "        result = Detection(bounding_box=bounding_box, categories=[category])\n",
    "        results.append(result)\n",
    "\n",
    "    # Sort detection results by score ascending\n",
    "    sorted_results = sorted(\n",
    "        results,\n",
    "        key=lambda detection: detection.categories[0].score,\n",
    "        reverse=True)\n",
    "\n",
    "    # Filter out detections in deny list\n",
    "    filtered_results = sorted_results\n",
    "    if self._options.label_deny_list is not None:\n",
    "      filtered_results = list(\n",
    "          filter(\n",
    "              lambda detection: detection.categories[0].label not in self.\n",
    "              _options.label_deny_list, filtered_results))\n",
    "\n",
    "    # Keep only detections in allow list\n",
    "    if self._options.label_allow_list is not None:\n",
    "      filtered_results = list(\n",
    "          filter(\n",
    "              lambda detection: detection.categories[0].label in self._options.\n",
    "              label_allow_list, filtered_results))\n",
    "\n",
    "    # Only return maximum of max_results detection.\n",
    "    if self._options.max_results > 0:\n",
    "      result_count = min(len(filtered_results), self._options.max_results)\n",
    "      filtered_results = filtered_results[:result_count]\n",
    "\n",
    "    return filtered_results\n",
    "\n",
    "\n",
    "_MARGIN = 10  # pixels\n",
    "_ROW_SIZE = 10  # pixels\n",
    "_FONT_SIZE = 1\n",
    "_FONT_THICKNESS = 1\n",
    "_TEXT_COLOR = (0, 0, 255)  # red\n",
    "\n",
    "\n",
    "def visualize(\n",
    "    image: np.ndarray,\n",
    "    detections: List[Detection],\n",
    ") -> np.ndarray:\n",
    "  \"\"\"Draws bounding boxes on the input image and return it.\n",
    "  Args:\n",
    "    image: The input RGB image.\n",
    "    detections: The list of all \"Detection\" entities to be visualize.\n",
    "  Returns:\n",
    "    Image with bounding boxes.\n",
    "  \"\"\"\n",
    "  for detection in detections:\n",
    "    # Draw bounding_box\n",
    "    start_point = detection.bounding_box.left, detection.bounding_box.top\n",
    "    end_point = detection.bounding_box.right, detection.bounding_box.bottom\n",
    "    cv2.rectangle(image, start_point, end_point, _TEXT_COLOR, 3)\n",
    "\n",
    "    # Draw label and score\n",
    "    category = detection.categories[0]\n",
    "    class_name = category.label\n",
    "    probability = round(category.score, 2)\n",
    "    result_text = class_name + ' (' + str(probability) + ')'\n",
    "    text_location = (_MARGIN + detection.bounding_box.left,\n",
    "                     _MARGIN + _ROW_SIZE + detection.bounding_box.top)\n",
    "    cv2.putText(image, result_text, text_location, cv2.FONT_HERSHEY_PLAIN,\n",
    "                _FONT_SIZE, _TEXT_COLOR, _FONT_THICKNESS)\n",
    "\n",
    "  return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "TEMP_FILE = \"/content/dataset/validation/IMG_0493.jpg\" #@param {type:\"string\"}\n",
    "DETECTION_THRESHOLD = 0.5 #@param {type:\"number\"}\n",
    "TFLITE_MODEL_PATH = \"android.tflite\" #@param {type:\"string\"}\n",
    "\n",
    "#TEMP_FILE = '/tmp/image.png'\n",
    "\n",
    "#!wget -q -O $TEMP_FILE $INPUT_IMAGE_URL\n",
    "image = Image.open(TEMP_FILE).convert('RGB')\n",
    "image.thumbnail((512, 512), Image.ANTIALIAS)\n",
    "image_np = np.asarray(image)\n",
    "\n",
    "# Load the TFLite model\n",
    "options = ObjectDetectorOptions(\n",
    "      num_threads=4,\n",
    "      score_threshold=DETECTION_THRESHOLD,\n",
    ")\n",
    "detector = ObjectDetector(model_path=\"./saved-model/\" + TFLITE_MODEL_PATH, options=options)\n",
    "\n",
    "# Run object detection estimation using the model.\n",
    "detections = detector.detect(image_np)\n",
    "\n",
    "# Draw keypoints and edges on input image\n",
    "image_np = visualize(image_np, detections)\n",
    "\n",
    "# Show the detection result\n",
    "Image.fromarray(image_np)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.13 (default, Oct 19 2022, 22:38:03) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6e58eff0d9e1b4699358d365480a19e254af2633293adbb99813241d21d32892"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
